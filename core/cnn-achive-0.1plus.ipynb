{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T09:56:47.255158Z",
     "start_time": "2018-11-04T09:56:47.242644Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import gensim\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Embedding, Dense, Conv1D, GlobalAveragePooling1D, Flatten, concatenate, Lambda, BatchNormalization, Dropout\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T06:10:26.434944Z",
     "start_time": "2018-11-04T06:10:25.989450Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(\"../models/word2vec.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T09:49:08.753592Z",
     "start_time": "2018-11-04T09:49:08.664159Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_conv1D(emb_matrix, input_len=60, magic_feature_num=5, distance_feature_num=20):\n",
    "    emb_layer = Embedding(input_dim=emb_matrix.shape[0], output_dim=emb_matrix.shape[1], input_length=input_len, \n",
    "                          trainable=False, embeddings_initializer=keras.initializers.Constant(emb_matrix))\n",
    "    input_a = Input(shape=(input_len, ))\n",
    "    input_b = Input(shape=(input_len, ))\n",
    "    emb_a = emb_layer(input_a)\n",
    "    emb_b = emb_layer(input_b)\n",
    "    sizes = [(128, 1), (128, 2), (128, 3), (128, 4), (32, 5), (32, 6)]\n",
    "    global_as = []\n",
    "    global_bs = []\n",
    "    for filter_size, kernel_size in sizes:\n",
    "        conv_a = Conv1D(filters=filter_size, kernel_size=kernel_size, padding=\"SAME\", activation=\"relu\")(emb_a)\n",
    "        global_a = GlobalAveragePooling1D()(conv_a)\n",
    "        conv_b = Conv1D(filters=filter_size, kernel_size=kernel_size, padding=\"SAME\", activation=\"relu\")(emb_b)\n",
    "        global_b = GlobalAveragePooling1D()(conv_b)\n",
    "        global_as.append(global_a)\n",
    "        global_bs.append(global_b)\n",
    "    merge_a = concatenate(global_as)\n",
    "    merge_b = concatenate(global_bs)\n",
    "    diff = Lambda(lambda x: K.abs(x[0] - x[1]), output_shape=(4 * 128 + 2 * 32, ))([merge_a, merge_b])\n",
    "    mul = Lambda(lambda x: [0] * x[1], output_shape=(4 * 128 + 2 * 32, ))([merge_a, merge_b])\n",
    "    \n",
    "    magic_input = Input(shape=(magic_feature_num, ))\n",
    "    magic_dense = BatchNormalization()(magic_input)\n",
    "    magic_dense = Dense(64, activation=\"relu\")(magic_dense)\n",
    "    \n",
    "    distance_input = Input(shape=(distance_feature_num, ))\n",
    "    distance_dense = BatchNormalization()(distance_input)\n",
    "    distance_dense = Dense(128, activation=\"relu\")(distance_input)\n",
    "    \n",
    "    merge = concatenate([diff, mul, magic_dense, distance_dense])\n",
    "    \n",
    "    x = Dropout(0.2)(merge)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(300, activation=\"relu\")(x)\n",
    "    \n",
    "    x = Dropout(0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    pred = Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model([input_a, input_b, magic_input, distance_input], outputs=pred)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T06:10:27.541554Z",
     "start_time": "2018-11-04T06:10:27.455765Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23550, 128)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.array([word2vec_model[w] for w in word2vec_model.wv.index2word])\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T08:30:10.907012Z",
     "start_time": "2018-11-04T08:30:08.994665Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"../datasets/train.csv\")\n",
    "train_data = train_data.fillna(\"\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T08:35:36.551689Z",
     "start_time": "2018-11-04T08:35:31.027129Z"
    }
   },
   "outputs": [],
   "source": [
    "train_token1 = train_data.question1.apply(lambda x: x.lower().split())\n",
    "train_token2 = train_data.question2.apply(lambda x: x.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T08:37:07.069871Z",
     "start_time": "2018-11-04T08:36:07.900553Z"
    }
   },
   "outputs": [],
   "source": [
    "test_token1 = np.load(\"../datasets/test_words1.npy\")\n",
    "test_token2 = np.load(\"../datasets/test_words2.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T08:57:14.784725Z",
     "start_time": "2018-11-04T08:57:14.773961Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LEN = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T09:03:50.154109Z",
     "start_time": "2018-11-04T09:03:50.124080Z"
    }
   },
   "outputs": [],
   "source": [
    "word2index_dict = {w: i for i, w in enumerate(word2vec_model.wv.index2word, 1)}\n",
    "\n",
    "def word2index(sequence):\n",
    "    return [word2index_dict.get(w, 0) for w in sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T09:08:42.344017Z",
     "start_time": "2018-11-04T09:07:58.226935Z"
    }
   },
   "outputs": [],
   "source": [
    "train_token_code1 = np.array([word2index(s) for s in train_token1])\n",
    "train_token_code2 = np.array([word2index(s) for s in train_token2])\n",
    "test_token_code1 = np.array([word2index(s) for s in test_token1])\n",
    "test_token_code2 = np.array([word2index(s) for s in test_token2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T09:21:49.200911Z",
     "start_time": "2018-11-04T09:21:09.426252Z"
    }
   },
   "outputs": [],
   "source": [
    "train_token_code1 = pad_sequences(train_token_code1, maxlen=MAX_SEQUENCE_LEN)\n",
    "train_token_code2 = pad_sequences(train_token_code2, maxlen=MAX_SEQUENCE_LEN)\n",
    "test_token_code1 = pad_sequences(test_token_code1, maxlen=MAX_SEQUENCE_LEN)\n",
    "test_token_code2 = pad_sequences(test_token_code2, maxlen=MAX_SEQUENCE_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T09:30:19.582600Z",
     "start_time": "2018-11-04T09:30:16.498277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "magic train: (404290, 3)\n",
      "magic test: (2345796, 3)\n"
     ]
    }
   ],
   "source": [
    "train_magic_v1 = pd.read_csv(\"../datasets/train_magic_feature_v1.csv\")\n",
    "train_magic_v2 = pd.read_csv(\"../datasets/train_magic_feature_v2.csv\")\n",
    "test_magic_v1 = pd.read_csv(\"../datasets/test_magic_feature_v1.csv\")\n",
    "test_magic_v2 = pd.read_csv(\"../datasets/test_magic_feature_v2.csv\")\n",
    "columns_v1 = ['q1_freq', 'q2_freq']\n",
    "columns_v2 = ['q1_q2_intersect']\n",
    "train_magic = pd.concat([train_magic_v1[columns_v1], train_magic_v2[columns_v2]], axis=1)\n",
    "test_magic = pd.concat([test_magic_v1[columns_v1], test_magic_v2[columns_v2]], axis=1)\n",
    "print(\"magic train:\", train_magic.shape)\n",
    "print(\"magic test:\", test_magic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T09:37:49.424739Z",
     "start_time": "2018-11-04T09:37:37.693809Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (404290, 18)\n",
      "test shape: (2345796, 18)\n"
     ]
    }
   ],
   "source": [
    "train_distance = pd.read_csv(\"../datasets/train_featured.csv\")\n",
    "test_distance = pd.read_csv(\"../datasets/test_featured_split.csv\")\n",
    "test_distance = test_distance.drop('diff_len', axis=1)\n",
    "print(\"train shape:\", train_distance.shape)\n",
    "print(\"test shape:\", test_distance.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_scaler = StandardScaler()\n",
    "train_distance_scaled = distance_scaler.fit_transform(train_distance)\n",
    "test_distance_scaled = distance_scaler.transform(test_distance)\n",
    "\n",
    "magic_scaler = StandardScaler()\n",
    "train_magic_scaled = magic_scaler.fit_transform(train_magic)\n",
    "test_magic_scaled = magic_scaler.transform(test_magic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T09:49:12.923630Z",
     "start_time": "2018-11-04T09:49:11.280774Z"
    }
   },
   "outputs": [],
   "source": [
    "model = model_conv1D(embedding_matrix, input_len=MAX_SEQUENCE_LEN, magic_feature_num=3, distance_feature_num=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T10:14:21.740258Z",
     "start_time": "2018-11-04T09:57:49.089233Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 363861 samples, validate on 40429 samples\n",
      "Epoch 1/30\n",
      "363861/363861 [==============================] - 154s 423us/step - loss: 0.2970 - acc: 0.8604 - val_loss: 0.2742 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.27421, saving model to ../models/cnn-1d-achive-0.1-v2.model\n",
      "Epoch 2/30\n",
      "363861/363861 [==============================] - 155s 427us/step - loss: 0.2747 - acc: 0.8721 - val_loss: 0.2532 - val_acc: 0.8842\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.27421 to 0.25323, saving model to ../models/cnn-1d-achive-0.1-v2.model\n",
      "Epoch 3/30\n",
      "363861/363861 [==============================] - 152s 419us/step - loss: 0.2663 - acc: 0.8767 - val_loss: 0.2543 - val_acc: 0.8820\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.25323\n",
      "Epoch 4/30\n",
      "363861/363861 [==============================] - 155s 427us/step - loss: 0.2601 - acc: 0.8803 - val_loss: 0.2527 - val_acc: 0.8834\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.25323 to 0.25269, saving model to ../models/cnn-1d-achive-0.1-v2.model\n",
      "Epoch 5/30\n",
      "363861/363861 [==============================] - 158s 433us/step - loss: 0.2541 - acc: 0.8826 - val_loss: 0.2490 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.25269 to 0.24903, saving model to ../models/cnn-1d-achive-0.1-v2.model\n",
      "Epoch 6/30\n",
      "363861/363861 [==============================] - 160s 438us/step - loss: 0.2502 - acc: 0.8849 - val_loss: 0.2452 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.24903 to 0.24516, saving model to ../models/cnn-1d-achive-0.1-v2.model\n",
      "Epoch 7/30\n",
      "363861/363861 [==============================] - 159s 436us/step - loss: 0.2447 - acc: 0.8874 - val_loss: 0.2436 - val_acc: 0.8892\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.24516 to 0.24363, saving model to ../models/cnn-1d-achive-0.1-v2.model\n",
      "Epoch 8/30\n",
      "363861/363861 [==============================] - 159s 437us/step - loss: 0.2411 - acc: 0.8894 - val_loss: 0.2420 - val_acc: 0.8890\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.24363 to 0.24204, saving model to ../models/cnn-1d-achive-0.1-v2.model\n",
      "Epoch 9/30\n",
      "363861/363861 [==============================] - 159s 436us/step - loss: 0.2355 - acc: 0.8922 - val_loss: 0.2429 - val_acc: 0.8897\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.24204\n",
      "Epoch 10/30\n",
      "363861/363861 [==============================] - 159s 437us/step - loss: 0.2328 - acc: 0.8942 - val_loss: 0.2425 - val_acc: 0.8898\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.24204\n",
      "Epoch 11/30\n",
      "363861/363861 [==============================] - 159s 438us/step - loss: 0.2282 - acc: 0.8959 - val_loss: 0.2411 - val_acc: 0.8906\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.24204 to 0.24106, saving model to ../models/cnn-1d-achive-0.1-v2.model\n",
      "Epoch 12/30\n",
      "363861/363861 [==============================] - 159s 437us/step - loss: 0.2235 - acc: 0.8986 - val_loss: 0.2387 - val_acc: 0.8920\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.24106 to 0.23865, saving model to ../models/cnn-1d-achive-0.1-v2.model\n",
      "Epoch 13/30\n",
      "363861/363861 [==============================] - 159s 437us/step - loss: 0.2207 - acc: 0.8996 - val_loss: 0.2398 - val_acc: 0.8910\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.23865\n",
      "Epoch 14/30\n",
      "363861/363861 [==============================] - 159s 436us/step - loss: 0.2173 - acc: 0.9011 - val_loss: 0.2391 - val_acc: 0.8907\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.23865\n",
      "Epoch 15/30\n",
      "363861/363861 [==============================] - 158s 435us/step - loss: 0.2133 - acc: 0.9031 - val_loss: 0.2447 - val_acc: 0.8894\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.23865\n",
      "Epoch 16/30\n",
      "363861/363861 [==============================] - 161s 442us/step - loss: 0.2097 - acc: 0.9050 - val_loss: 0.2441 - val_acc: 0.8874\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.23865\n",
      "Epoch 17/30\n",
      "363861/363861 [==============================] - 160s 439us/step - loss: 0.2061 - acc: 0.9067 - val_loss: 0.2424 - val_acc: 0.8916\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.23865\n",
      "Epoch 00017: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f86c7c26850>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = train_data.is_duplicate.values\n",
    "early_stopping = EarlyStopping(patience=5, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint(\"../models/cnn-1d-achive-0.1-v2.model\", verbose=1, save_best_only=True)\n",
    "model.fit([train_token_code1, train_token_code2, train_magic_scaled, train_distance_scaled], y, batch_size=128, \n",
    "          epochs=30, validation_split=0.1, callbacks=[early_stopping, model_checkpoint])\n",
    "# , class_weight={0: 1.309028344, 1: 0.472001959}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"../models/cnn-1d-achive-0.1-v2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2345796/2345796 [==============================] - 292s 124us/step\n"
     ]
    }
   ],
   "source": [
    "submission = model.predict([test_token_code1, test_token_code2, test_magic_scaled, test_distance_scaled], batch_size=512, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>test_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.009247</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.381514</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.376577</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001449</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.306953</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_duplicate  test_id\n",
       "0      0.009247        0\n",
       "1      0.381514        1\n",
       "2      0.376577        2\n",
       "3      0.001449        3\n",
       "4      0.306953        4"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submission = pd.DataFrame({'is_duplicate': submission.reshape(-1, ), 'test_id': range(len(submission))})\n",
    "df_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419114.5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.reshape(-1, ).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404290/404290 [==============================] - 49s 122us/step\n"
     ]
    }
   ],
   "source": [
    "train_pred = model.predict([train_token_code1, train_token_code2, train_magic, train_distance], batch_size=512, verbose=1).reshape(-1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.9996662e-01, 2.0979383e-23, 4.5031977e-22, ..., 3.0439164e-28,\n",
       "       4.6619011e-33, 9.0071702e-01], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6450295579905513"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((train_pred >= 0.5) == train_data.is_duplicate.values) / train_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314384"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(submission.reshape(-1, ) >= 0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.to_csv(\"../predictions/cnn-1d-v2.csv.gz\", index=False, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-py2",
   "language": "python",
   "name": "tensorflow-py2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
